---
layout: default
---



# Understanding Local AI (1st day of incubation)
## Ollama
on the first day of the incubation, we were given a presentation on the IAC and then what it does, going over how it has leveraged peoples and interest in AI development and provided them with the facilities needed to train their own AI models. Then we were introduced to two AI and software developers that would help us in our incubation and learning. With both their help, I properly understood the limitations and the amount of resources that is required to fully tain your own LLM form scratch. What they recommended was rather than train your AI from scratch, use an preexisting ai and then fine tune it to suit your needs.They also mentioned using the Rag pipeline which would be the most viable option for training our ai model.

After the presentation, we were assigned a room where we could work and learn how to finetune our own ai models.

Firstly before anything started I watched some tutorials on how to run ai locally. The reason I did this was so that I could witness the amount of AI models that existed and could be used for my project. This the link to the ollama website --> [Ollama](https://ollama.com/).

Then I downloaded Ollama onto my laptop. Before that, I also browsed through the ollama library which was full of multiple AI models, each trained on unique data sets, different requirements such as ram and also with different amount of parameters. With this vast Library of different AI models, I was free to choose which ever one that I thought was the best for my project. I chose the llama 3:8b which took 8 GB of space on my laptop and tried running it locally on my laptop.

## Reality 

After downloading the llamma 3:8b model onto my laptop, I tried to run it. But it didn't work as my laptop only had 1.6 GB worth of Ram Left for application processing where as for llama 3:8b, the minimum Ram required was 6.2 GB. 

After that, I had no choice but to download another model that was compact and also suitable for my laptop. I chose the Gemma3:1b model which required only 800 MB of Ram to operate. After that I opened ollama and then chatted with the model locally (offline). 

The first day was filled with trails and errors, with me realizing the sheer amount of Ram that was required to run normal AI models. Though it was a decent start to the month as I started to understand the diversity of ai models, each with their own perks such as deep comprehension, or the the Gemma3:1b models compact and space efficient application.

I was opened to the fact that there are several models to choose form and which ever one is best suited for my requirements. But there was an issue that had no way to be fixed, the Wifi wasn't really the best. therefore downloading and models took a long while. Due to that the entire day was spent on learning, understanding and experimenting with different AI models. That was all that happened on the 12th of January(1st day of Incubation)