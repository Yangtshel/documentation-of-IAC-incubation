<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Documentation | Home</title>
    <style>
        /* 1. Global Reset & Body */
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            margin: 0;
            padding: 0;
            background-color: #f6f8fa;
        }

        /* 2. Navigation Bar (Top) */
        nav {
            background-color: #24292e;
            padding: 1rem 0;
            text-align: center;
        }
        nav a {
            color: #ffffff; /* White text for Nav */
            margin: 0 15px;
            text-decoration: none;
            font-weight: 500;
        }
        nav a:hover {
            color: #58a6ff; /* Light blue on hover */
        }

        /* 3. The Anti-Edge Container */
        .content-wrapper {
            max-width: 850px;      
            margin: 40px auto;     
            padding: 0 30px;       /* Keeps text away from screen edges */
            background-color: #ffffff;
            border: 1px solid #d0d7de;
            border-radius: 6px;
        }

        .inner-content {
            padding: 40px 20px;
        }

        /* 4. CLEAN LINK STYLING (No Dirty Blue) */
        .inner-content a {
            color: #24292e;         /* Same color as text (Charcoal) */
            text-decoration: underline;
            text-decoration-color: #d0d7de; /* Light grey underline */
            transition: 0.2s;       /* Smooth color change */
        }

        .inner-content a:hover {
            color: #0969da;         /* Turns Professional Blue only on hover */
            text-decoration-color: #0969da;
        }

        .inner-content a:visited {
            color: #24292e;         /* Prevents that purple "visited" color */
        }

        h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 10px; }
        
        @media (max-width: 600px) {
            .content-wrapper { margin: 10px; border: none; }
            nav a { display: block; margin: 10px 0; }
        }
    </style>
</head>
<body>

    <!-- Top Navigation -->
    <nav>
        <a href="https://yangtshel.github.io/documentation-of-IAC-incubation/">Home</a>
        <!-- Ensure these file paths match your folder structure -->
        <a href="https://yangtshel.github.io/documentation-of-IAC-incubation/Week_1/navigationday.html">Week 1</a>
        <a href="/Week_2/navigationday.html">Week 2</a>
    </nav>
    
    <!-- The Wrapper that keeps text centered and away from edges -->
    <div class="content-wrapper">
        <div class="inner-content">
            <h1 id="detailed-fintuning-day-4-">Detailed fintuning (day 4)</h1>
            <h2 id="recap">Recap</h2>
            <p>Yesterday I officially finetuned my first ai model using google colab and the free GPU usage that it provided. With that I was able to fine tuned the Gemma3:1b on the PDF document of the learning process of the Bhutan Baccaluareate. After the finetuning we downloaded the GGUF file and then used ollama in our terminal to start and run it. </p>
            <p>Though after chatting with it for a few minutes I realised that the model would start hallucinating after the first few questions. What the fintuning did was merely catch the vibe of the of the PDF document.</p>
            <h2 id="plan">Plan</h2>
            <p>For today I want to deeply finetune an ai model so that it would capture more of the information in the PDF document and give more accurate responses. Therefore I would have to make a few adjustments to the training process that ensures that the model is finetuned deeply.</p>
            <p>Firstly, we have to increase the number of steps, meaning we are simply training it mroe, then change the training precision making it more detailed and lastly by also adding a val file(validation file) along side my Json(PDF converted to JSON) to ensure that my that the model is learning precisely what it is supposed to learn and simply doesn&#39;t start absorbing and being finetuned upon rubbish.</p>
            <p>With all of that in mind, i simply need to tinker a bit with my original code so that I can get all the above done. Though I did see that making all these changes would make it take a very long while, even for the google colab provided T4 GPU.</p>
            <h2 id="adjusting-python-code">adjusting  python code</h2>
            <p>I will first start of with importing a few libraries and ensure that my new code works properly.</p>
            <pre><code class="lang-python">%pip <span class="hljs-keyword">install</span> -U trl transformers accelerate datasets bitsandbytes peft
            <span class="hljs-keyword">from</span> google.colab <span class="hljs-keyword">import</span> drive
            drive.mount(<span class="hljs-string">'/content/drive'</span>)
            # <span class="hljs-keyword">Use</span> quotes <span class="hljs-keyword">if</span> your filename has spaces <span class="hljs-keyword">or</span> special <span class="hljs-keyword">characters</span>
            !cp <span class="hljs-string">"gemma-3-1b-it.Q4_K_M.gguf"</span> <span class="hljs-string">"/content/drive/MyDrive/"</span>
            </code></pre>
            <p>This code prepares a Google Colab environment to save a specific large language model file to my personal Google Drive for long-term storage or deployment. It begins by installing and updating the necessary Python libraries for modern AI development—specifically TRL, Transformers, and BitsAndBytes—which are required for handling and quantizing models. After mounting your Google Drive to the Colab instance, it uses a shell command (!cp) to copy a Gemma 3 1B model file (in the compressed GGUF format) from the local environment to your Drive&#39;s root folder, ensuring that you don&#39;t lose the model once your temporary Colab session ends.</p>
            <p>It also allows to save my progress on training as even if its intrarupted, I can simply save the progress on my google drive.</p>
            <pre><code class="lang-python">from unsloth <span class="hljs-built_in">import</span> FastLanguageModel
            <span class="hljs-built_in">import</span> torch
            from trl <span class="hljs-built_in">import</span> SFTTrainer
            from transformers <span class="hljs-built_in">import</span> TrainingArguments
            from datasets <span class="hljs-built_in">import</span> load_dataset

            <span class="hljs-comment"># 1. Load the model</span>
            model, <span class="hljs-attr">tokenizer</span> = FastLanguageModel.from_pretrained(
                <span class="hljs-attr">model_name</span> = <span class="hljs-string">"unsloth/gemma-3-1b-it"</span>,
                <span class="hljs-attr">max_seq_length</span> = <span class="hljs-number">2048</span>,
                <span class="hljs-attr">load_in_4bit</span> = True,
            )

            <span class="hljs-comment"># 2. Higher Rank (r=32) for better learning depth</span>
            <span class="hljs-attr">model</span> = FastLanguageModel.get_peft_model(
                model,
                <span class="hljs-attr">r</span> = <span class="hljs-number">64</span>,
                <span class="hljs-attr">target_modules</span> = [<span class="hljs-string">"q_proj"</span>, <span class="hljs-string">"k_proj"</span>, <span class="hljs-string">"v_proj"</span>, <span class="hljs-string">"o_proj"</span>, <span class="hljs-string">"gate_proj"</span>, <span class="hljs-string">"up_proj"</span>, <span class="hljs-string">"down_proj"</span>],
                <span class="hljs-attr">lora_alpha</span> = <span class="hljs-number">32</span>,
                <span class="hljs-attr">lora_dropout</span> = <span class="hljs-number">0.05</span>,
                <span class="hljs-attr">bias</span> = <span class="hljs-string">"none"</span>,
            )
            </code></pre>
            <p>Now this code will sets up a high-capacity training environment to fine-tune the Gemma 3 (1B) model using the Unsloth library for maximum hardware efficiency. It loads the model in 4-bit quantization to save memory while doubling the context window to 2048 tokens, allowing the model to process significantly longer passages of text. By configuring a high LoRA rank of (r=64) and an alpha of 32, the script allocates a larger number of trainable parameters than standard setups, enabling the model to learn more complex patterns and deeper nuances from the training data, while the inclusion of a 0.05 dropout helps prevent the model from overfitting during this more intensive learning process.</p>
            <p>Though as with these things also comes drawbacks, it takes a really long time to process and  </p>
            <pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
            <span class="hljs-keyword">import</span> json

            <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">force_fix_jsonl</span><span class="hljs-params">(input_file, output_file)</span>:</span>
                <span class="hljs-keyword">with</span> open(input_file, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
                    content = f.read()
                text = content.decode(<span class="hljs-string">'utf-16'</span>) <span class="hljs-keyword">if</span> <span class="hljs-string">b'\xff\xfe'</span> <span class="hljs-keyword">in</span> content[:<span class="hljs-number">2</span>] <span class="hljs-keyword">or</span> <span class="hljs-string">b'\xfe\xff'</span> <span class="hljs-keyword">in</span> content[:<span class="hljs-number">2</span>] <span class="hljs-keyword">else</span> content.decode(<span class="hljs-string">'utf-8'</span>, errors=<span class="hljs-string">'ignore'</span>)
                text = text.replace(<span class="hljs-string">'\x00'</span>, <span class="hljs-string">''</span>)
                <span class="hljs-keyword">with</span> open(output_file, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f_out:
                    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> text.splitlines():
                        line = line.strip()
                        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> line: <span class="hljs-keyword">continue</span>
                        <span class="hljs-keyword">try</span>:
                            json.loads(line)
                            f_out.write(line + <span class="hljs-string">'\n'</span>)
                        <span class="hljs-keyword">except</span>: <span class="hljs-keyword">continue</span>

            <span class="hljs-comment"># Fix and load</span>
            force_fix_jsonl(<span class="hljs-string">'train.jsonl'</span>, <span class="hljs-string">'train_fixed.jsonl'</span>)
            full_dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files={<span class="hljs-string">"train"</span>: <span class="hljs-string">"train_fixed.jsonl"</span>}, split=<span class="hljs-string">"train"</span>)
            dataset = full_dataset.train_test_split(test_size=<span class="hljs-number">0.1</span>, seed=<span class="hljs-number">42</span>)

            <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">formatting_prompts_func</span><span class="hljs-params">(examples)</span>:</span>
                texts = [f<span class="hljs-string">"&lt;start_of_turn&gt;user\n{inst}&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\n{out}&lt;end_of_turn&gt;"</span>
                        <span class="hljs-keyword">for</span> inst, out <span class="hljs-keyword">in</span> zip(examples[<span class="hljs-string">"instruction"</span>], examples[<span class="hljs-string">"response"</span>])]
                <span class="hljs-keyword">return</span> { <span class="hljs-string">"text"</span> : texts }

            train_data = dataset[<span class="hljs-string">"train"</span>].map(formatting_prompts_func, batched = <span class="hljs-keyword">True</span>).shuffle(seed=<span class="hljs-number">42</span>)
            val_data = dataset[<span class="hljs-string">"test"</span>].map(formatting_prompts_func, batched = <span class="hljs-keyword">True</span>)
            </code></pre>
            <p>This python code performs a complex and robust data-cleaning and preprocessing pipeline to prepare a custom dataset for fine-tuning a Gemma 3 model. It first begins with a &quot;force-fix&quot; function that handles corrupt JSONL files by resolving encoding issues (like UTF-16 vs. UTF-8), stripping null characters, and discarding malformed lines to ensure only valid JSON remains. After loading this cleaned data, the script splits it into a 90% training and 10% validation set before applying a formatting function that maps raw instruction-response pairs into the specific <start_of_turn> and <end_of_turn> prompt template required by Gemma. The final result is a shuffled, token-ready dataset structured for Supervised Fine-Tuning (SFT).</p>
            <p>This Ensures that the data and validation files are of high quality before the finfetuning even beings.</p>
            <pre><code class="lang-python">from trl <span class="hljs-built_in">import</span> SFTTrainer
            from transformers <span class="hljs-built_in">import</span> TrainingArguments

            <span class="hljs-attr">trainer</span> = SFTTrainer(
                <span class="hljs-attr">model</span> = model,
                <span class="hljs-attr">tokenizer</span> = tokenizer,
                <span class="hljs-attr">train_dataset</span> = train_data,
                <span class="hljs-attr">eval_dataset</span> = val_data,
                <span class="hljs-attr">dataset_text_field</span> = <span class="hljs-string">"text"</span>,
                <span class="hljs-attr">max_seq_length</span> = <span class="hljs-number">1024</span>,
                <span class="hljs-attr">args</span> = TrainingArguments(
                    <span class="hljs-attr">per_device_train_batch_size</span> = <span class="hljs-number">2</span>,
                    <span class="hljs-attr">gradient_accumulation_steps</span> = <span class="hljs-number">4</span>,
                    <span class="hljs-attr">max_steps</span> = <span class="hljs-number">1000</span>,
                    <span class="hljs-attr">learning_rate</span> = <span class="hljs-number">2</span>e-<span class="hljs-number">4</span>,
                    <span class="hljs-attr">fp16</span> = not torch.cuda.is_bf16_supported(),
                    <span class="hljs-attr">bf16</span> = torch.cuda.is_bf16_supported(),
                    <span class="hljs-attr">logging_steps</span> = <span class="hljs-number">10</span>,
                    <span class="hljs-attr">eval_strategy</span> = <span class="hljs-string">"steps"</span>,  <span class="hljs-comment"># Fixed: changed from evaluation_strategy</span>
                    <span class="hljs-attr">eval_steps</span> = <span class="hljs-number">100</span>,
                    <span class="hljs-attr">optim</span> = <span class="hljs-string">"adamw_8bit"</span>,
                    <span class="hljs-attr">output_dir</span> = <span class="hljs-string">"outputs"</span>,
                    <span class="hljs-attr">report_to</span> = <span class="hljs-string">"none"</span>,
                ),
            )

            <span class="hljs-comment"># Clear memory before starting</span>
            <span class="hljs-built_in">import</span> torch
            torch.cuda.empty_cache()

            trainer.train()
            </code></pre>
            <p>This code executes the actual training process by combining the model, tokenizer, and cleaned datasets into a Supervised Fine-Tuning (SFT) Trainer. It uses a batch size of 2 with 4 accumulation steps to simulate a larger, more stable batch size of 8, while employing the 8-bit AdamW optimizer and Mixed Precision (bf16/fp16) to minimize GPU memory usage. The trainer is configured to run for 1,000 steps, providing performance logs every 10 steps and running a validation check every 100 steps to monitor for overfitting. Just before starting the trainer.train() command, it clears the GPU cache to ensure there is maximum available VRAM for the optimization process.</p>
            <p>With the execution of that final code block the waiting game begins. While that was running I noticed that per 100 steps, it took around on average 48 minutes. Thus for it to finish all 1000 steps it would take roughly around ~8hours!!!! I started the code on Thusday 11:34 and waited. Also on top of the teidous wait time, I also had to manually be near my laptop to keep it form timing out. Google is really strict with its GPUs therefore to keep it away form being used by bots, it ensures a time out after 15 minutes of inactiveness. Thus I constantly had to be near my computer and keep fidling with my mouse pad to ensure it doesnt timeout for the next 8 hours!</p>
            <h2 id="hard-reality">Hard Reality</h2>
            <p>At around 3 PM in the evening, suddenly a notification popd up on my screen. It said, &quot;Runner Timeout&quot; I thought that I had messed up and let it sit inactive for more than 15 minutes. But when I tried restarting the code, it still appread. I was really confused and tried researching a bit about it.</p>
            <p>Turns out that there is a limit for how many hour I could use the GPU provided by Google. And I  apprantly had hit that limit. Meaning that I wouldnt be allowed to use their GPUs any longer. Thus I could use google colab for any of my future finetuning.</p>


            
        </div>
    </div>

</body>
</html>

