<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Documentation | Home</title>
    <style>
        /* 1. Global Styles */
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            margin: 0;
            padding: 0;
            background-color: #f6f8fa; /* Light grey background */
        }

        /* 2. Navigation Bar */
        nav {
            background-color: #24292e; /* Dark GitHub-style header */
            padding: 1rem 0;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        nav a {
            color: #ffffff;
            margin: 0 15px;
            text-decoration: none;
            font-weight: 500;
            font-size: 1.1rem;
        }
        nav a:hover {
            color: #58a6ff; /* Light blue hover effect */
        }

        /* 3. The "Anti-Edge" Container */
        .content-wrapper {
            max-width: 850px;      /* Stops text from stretching too far */
            margin: 40px auto;     /* Centers the box and adds top/bottom space */
            padding: 0 30px;       /* This ensures text NEVER touches the edge */
            background-color: #ffffff; /* White box for content */
            border: 1px solid #d0d7de;
            border-radius: 6px;
        }

        /* 4. Content Styling */
        .inner-content {
            padding: 40px 20px;    /* Extra space inside the white box */
        }
        h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 10px; }
        code { background: #f1f1f1; padding: 2px 5px; border-radius: 3px; }
        
        /* Mobile adjustment */
        @media (max-width: 600px) {
            .content-wrapper { margin: 10px; border: none; }
            nav a { display: block; margin: 10px 0; }
        }
    </style>
</head>
<body>

    <!-- Top Navigation -->
    <nav>
        <a href="index.html">Home</a>
        <!-- Ensure these file paths match your folder structure -->
        <a href="your-folder/file1.html">Assignment 1</a>
        <a href="your-folder/file2.html">Assignment 2</a>
    </nav>

    <!-- The Wrapper that keeps text centered and away from edges -->
    <div class="content-wrapper">
        <div class="inner-content">
            # Understanding Local AI (1st day of incubation)
            ## Ollama
            on the first day of the incubation, we were given a presentation on the IAC and then what it does, going over how it has leveraged peoples and interest in AI development and provided them with the facilities needed to train their own AI models. Then we were introduced to two AI and software developers that would help us in our incubation and learning. With both their help, I properly understood the limitations and the amount of resources that is required to fully tain your own LLM form scratch. What they recommended was rather than train your AI from scratch, use an preexisting ai and then fine tune it to suit your needs.They also mentioned using the Rag pipeline which would be the most viable option for training our ai model.

            After the presentation, we were assigned a room where we could work and learn how to finetune our own ai models.

            Firstly before anything started I watched some tutorials on how to run ai locally. The reason I did this was so that I could witness the amount of AI models that existed and could be used for my project. This the link to the ollama website --> [Ollama](https://ollama.com/).

            Then I downloaded Ollama onto my laptop. Before that, I also browsed through the ollama library which was full of multiple AI models, each trained on unique data sets, different requirements such as ram and also with different amount of parameters. With this vast Library of different AI models, I was free to choose which ever one that I thought was the best for my project. I chose the llama 3:8b which took 8 GB of space on my laptop and tried running it locally on my laptop.

            ## Reality 

            After downloading the llamma 3:8b model onto my laptop, I tried to run it. But it didn't work as my laptop only had 1.6 GB worth of Ram Left for application processing where as for llama 3:8b, the minimum Ram required was 6.2 GB. 

            After that, I had no choice but to download another model that was compact and also suitable for my laptop. I chose the Gemma3:1b model which required only 800 MB of Ram to operate. After that I opened ollama and then chatted with the model locally (offline). 

            The first day was filled with trails and errors, with me realizing the sheer amount of Ram that was required to run normal AI models. Though it was a decent start to the month as I started to understand the diversity of ai models, each with their own perks such as deep comprehension, or the the Gemma3:1b models compact and space efficient application.

            I was opened to the fact that there are several models to choose form and which ever one is best suited for my requirements. But there was an issue that had no way to be fixed, the Wifi wasn't really the best. therefore downloading and models took a long while. Due to that the entire day was spent on learning, understanding and experimenting with different AI models. That was all that happened on the 12th of January(1st day of Incubation)
                        
            
        </div>
    </div>

</body>
</html>


